{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d366e0dc",
   "metadata": {},
   "source": [
    "__packages necessary:__ (comment out if done through terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10430f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip setuptools\n",
    "%pip install numba llvmlite --user \n",
    "%pip install pandas\n",
    "%pip install tqdm\n",
    "%pip install scikit-learn\n",
    "%pip install torch\n",
    "%pip install tensorflow\n",
    "%pip install alibi==0.9.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11274318",
   "metadata": {},
   "source": [
    "__Necessary imports:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66702f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from alibi.explainers import AnchorTabular\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.errors import ParserError\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6d3ea",
   "metadata": {},
   "source": [
    "# Anchor explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef8663",
   "metadata": {},
   "source": [
    "__Code for anchor explanation using text of the title example:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246794e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.errors import ParserError\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and sample dataset early to save memory\n",
    "try:\n",
    "    df = pd.read_csv('WELFake_Dataset.csv', engine='python')\n",
    "except ParserError:\n",
    "    # skip lines causing errors ( this will only be a couple of instances)\n",
    "    print(\"ParserError encountered with python engine, trying to skip bad lines.\")\n",
    "    df = pd.read_csv('WELFake_Dataset.csv', engine = 'python', on_bad_lines='skip')\n",
    "#df = pd.read_csv('WELFake_Dataset.csv')\n",
    "df = df.dropna(subset=['title', 'text', 'label'])\n",
    "\n",
    "# Sample smaller subset for speed/memory\n",
    "df = df.sample(n=1500, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Limit to first 150 words per article to shorten input length\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(str(x).split()[:150]))\n",
    "#df['title'] = df['title'].apply(lambda x: ' '.join(str(x).split()[:150]))\n",
    "\n",
    "# Extract features and labels\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(tqdm(X, desc=\"Vectorizing\"))\n",
    "\n",
    "# Train-test split\n",
    "\n",
    "original_indices = np.arange(len(X)) # this allows to track original indices before split\n",
    "X_train, X_test, y_train, y_test, train_idx, test_idx= train_test_split(X_vec, y, original_indices, test_size=0.2, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prepare input for explanation\n",
    "X_dense = X_vec.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "index_to_explain = 4\n",
    "instance = X_test_dense[index_to_explain].reshape(1, -1)\n",
    "true_index = test_idx[index_to_explain]\n",
    "\n",
    "\n",
    "predict_fn = lambda x: clf.predict(x)\n",
    "\n",
    "# Anchor explanation\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "class_names = ['real', 'fake']\n",
    "\n",
    "explainer = AnchorTabular(predict_fn, feature_names)\n",
    "explainer.fit(X_dense[:500])\n",
    "\n",
    "print(f\"True class: {class_names[int(y_test[index_to_explain])]}\")\n",
    "print(f\"Title of explained instance: {df.loc[true_index, 'title']}\")\n",
    "print('\\nPrediction:(0 = real news, 1=  fake news): ', predict_fn(instance)[0])\n",
    "#print('\\nPrediction:(0 = real news, 1=  fake news): ', clf.predict(instance)[0])\n",
    "\n",
    "explanation = explainer.explain(\n",
    "    instance,\n",
    "    coverage_samples=100,\n",
    "    beam_size=3,\n",
    "    max_anchor_size=7,\n",
    "    stop_on_first=True\n",
    ")\n",
    "\n",
    "print(f\"\\nAnchor: {explanation.anchor}\")\n",
    "print(f\"Precision: {explanation.precision}\")\n",
    "print(f\"Coverage: {explanation.coverage}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal time taken: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b1005",
   "metadata": {},
   "source": [
    "__code for Anchor explainer that explains predictions based on titles (used in report):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65548664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = df['title'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(tqdm(X, desc=\"Vectorizing\"))\n",
    "\n",
    "# Train-test split\n",
    "original_indices = np.arange(len(X)) # this allows to track original indices before split\n",
    "X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(X_vec, y, original_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prepare input for explanation\n",
    "X_dense = X_vec.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "\n",
    "index_to_explain = 4\n",
    "#instance = X_dense[index_to_explain].reshape(1, -1)\n",
    "instance = X_test_dense[index_to_explain].reshape(1, -1)\n",
    "true_index = test_idx[index_to_explain]\n",
    "\n",
    "predict_fn = lambda x: clf.predict(x)\n",
    "\n",
    "# Anchor explanation\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "class_names = ['real', 'fake']\n",
    "\n",
    "explainer = AnchorTabular(predict_fn, feature_names)\n",
    "explainer.fit(X_dense[:500])\n",
    "\n",
    "print(f\"True class: {class_names[int(y_test[index_to_explain])]}\")\n",
    "print(f\"Title of explained instance: {df.loc[true_index, 'title']}\")\n",
    "print('\\nPrediction:(0 = real news, 1=  fake news): ', predict_fn(instance)[0])\n",
    "\n",
    "explanation = explainer.explain(\n",
    "    instance,\n",
    "    coverage_samples=100,\n",
    "    beam_size=3,\n",
    "    max_anchor_size=7,\n",
    "    stop_on_first=True\n",
    ")\n",
    "\n",
    "print(f\"\\nAnchor: {explanation.anchor}\")\n",
    "print(f\"Precision: {explanation.precision}\")\n",
    "print(f\"Coverage: {explanation.coverage}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60e465",
   "metadata": {},
   "source": [
    "# Attention-maps / attention based explanations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f300975",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b35dbd2b",
   "metadata": {},
   "source": [
    "__Code for training and plot of attention for example sentence__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ade0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model (bert-base-uncased) is not trained for prediction, so it just shows a random prediction and the output weights of the hidden state of the model ( which just represents how much semantical weight words have)\n",
    "# this is not an accurate representation of the attention the model would have for prediction of real vs fake, but it does show what a visualisation of attention of real vs fake news classification would look like\n",
    "# an attention-based explanation would look like. ( we can use the visualisation as demonstration of how it aligns with human intuition, therefore I still used it)\n",
    "#\n",
    "from transformers import BertConfig\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('WELFake_Dataset.csv', engine='python')\n",
    "except ParserError:\n",
    "    # skip lines causing errors ( this will only be a couple of instances)\n",
    "    print(\"ParserError encountered with python engine, trying to skip bad lines.\")\n",
    "    df = pd.read_csv('WELFake_Dataset.csv', engine = 'python', on_bad_lines='skip')\n",
    "   \n",
    "    \n",
    "df = df.dropna(subset=['title', 'text', 'label'])  # make sure no missing data\n",
    "\n",
    "df = df.sample(n=1500, random_state=42).reset_index(drop=True) # not necessary for this method, but still done so explanation can be shown for the same example instance\n",
    "\n",
    "X = df['title'].values\n",
    "y = df['label'].values  # assume binary: 1 = fake, 0 = real\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "index_explained_text = 4\n",
    "text_to_explain = X_test[index_explained_text]\n",
    "text_to_explain = text_to_explain[:1500] # limits long title to max length\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", output_hidden_states = True, output_attentions = True)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config = config)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = text_to_explain # makesit so\n",
    "\n",
    "# Tokenize and pass through model\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits  # Raw prediction scores\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "\n",
    "last_hidden_states = outputs.hidden_states\n",
    "print(\"\\n\",last_hidden_states)\n",
    "\n",
    "# Getting attention maps\n",
    "attention = outputs.attentions  # List of layers with attention weights\n",
    "attention_map = attention[-1][0]  # Last layer's attention for first head\n",
    "\n",
    "# Visualizing attention to [CLS] token\n",
    "cls_attention = attention_map[0][0].detach().numpy()\n",
    "print(cls_attention)  # Attention scores from [CLS] to each token\n",
    "\n",
    "\n",
    "\n",
    "# Prepare tokens for labeling\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "\n",
    "#Plots the attention map to visualize how much the model attends to the [CLS] token\n",
    "plt.figure(figsize=(18, 9))\n",
    "sns.heatmap(cls_attention.reshape(1, -1), cmap='Blues', cbar_kws={'label': 'Attention Score'}, xticklabels=tokens, yticklabels=[\"[CLS]\"], cbar=True)\n",
    "#sns.heatmap(cls_attention.reshape(1, -1),cmap = 'YlOrBr', cbar_kws={'label': 'Attention Score'}, xticklabels=tokens, yticklabels=[\"[CLS]\"], cbar=True)\n",
    "plt.title('Attention Map for [CLS] Token')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Attention to [CLS]')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#print(\"Label:\", \"fake\" if y_test[index_explained_text] == 1 else \"real\")\n",
    "if y_test[index_explained_text] == 0:\n",
    "    print(\"according to the dataset, this post included or was real news\")\n",
    "else:\n",
    "    print(\"According to the dataset, this post inclucded or was fake news\")\n",
    "print(\"example prediction:\", \"fake\" if predicted_class == 1 else \"real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9fdbf",
   "metadata": {},
   "source": [
    "__Code for visualisation of example sentence used in report(sorted by attention to [CLS])__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f913f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_token_scores = []\n",
    "attention_new = cls_attention.copy()\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "  if token in string.punctuation: # punctuation marks are just biased and show up in every text\n",
    "    continue\n",
    "  filtered_token_scores.append((token, attention_new[i]))\n",
    "\n",
    "sorted_tokens = sorted(filtered_token_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Formatting print to look like table\n",
    "print(f\"sorted attention scores from [CLS] token:\\n\")\n",
    "print(f\"{'Token':<15} {'Score'}\")\n",
    "print(\"-\" * 25) # for structure to separate scores and variables\n",
    "for token, score in sorted_tokens:\n",
    "      print(f\"{token:<15} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ed65f",
   "metadata": {},
   "source": [
    "__"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
